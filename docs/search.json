[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "ST558 Final Project - Modeling",
    "section": "",
    "text": "In these analyses, we will evaluate several predictive models using data on risk factors for diabetes from the 2015 Behavioral Risk Factor Surveillance System (BRFSS). Data are collected annually by phone using random digit dialing across all 50 states, resulting in over 250,000 adults over age 18 providing information about health conditions, health behaviors, and healthcare use. For the current analyses, we focus on the curated binary Diabetes Health Indicator Dataset. This dataset includes the variable “Diabetes_binary”, which is coded as 1 to indicate a respondent has been told they have diabetes, and 0 if they have not. This variable will serve as the primary outcome for our predictions.\nThe Diabetes Health Indicator Dataset also includes a subset of additional variables selected from the broader survey based on their relevance to diabetes risk, which we will use for our predictive models. These include 5 variables pertaining to objective health measures and/or conditions (high cholesterol, high blood pressure, BMI, history of stroke, and history of heart disease); 5 variables pertaining to health behaviors (history of smoking, heavy alcohol consumption, regular physical activity, eating daily vegetables, and eating daily fruits); 3 demographic variables (sex, age, and education); and 3 subjective health measures (days of poor mental health in past month, days of poor physical health in the past month, and having difficulty walking).\nWe will test 3 different predictive models, each using 3 different statistical approaches. Our first model will consider objective indices of health conditions plus demographics (8 variables total). Our second model will retain demographic variables but will replace objective health conditions with health behaviors. Finally, our third model will include all predictors described above, including demographics, health conditions, health behaviors, and perceived health.\nWe will begin with a subset of 70% of the data to train each of our models using logistic regression, classification trees, and random forest plots with 5-fold cross-validation. Then, we will apply our models to the remaining 30% of the data to test which model does the best at predicting our diabetes outcome."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "ST558 Final Project - Modeling",
    "section": "",
    "text": "In these analyses, we will evaluate several predictive models using data on risk factors for diabetes from the 2015 Behavioral Risk Factor Surveillance System (BRFSS). Data are collected annually by phone using random digit dialing across all 50 states, resulting in over 250,000 adults over age 18 providing information about health conditions, health behaviors, and healthcare use. For the current analyses, we focus on the curated binary Diabetes Health Indicator Dataset. This dataset includes the variable “Diabetes_binary”, which is coded as 1 to indicate a respondent has been told they have diabetes, and 0 if they have not. This variable will serve as the primary outcome for our predictions.\nThe Diabetes Health Indicator Dataset also includes a subset of additional variables selected from the broader survey based on their relevance to diabetes risk, which we will use for our predictive models. These include 5 variables pertaining to objective health measures and/or conditions (high cholesterol, high blood pressure, BMI, history of stroke, and history of heart disease); 5 variables pertaining to health behaviors (history of smoking, heavy alcohol consumption, regular physical activity, eating daily vegetables, and eating daily fruits); 3 demographic variables (sex, age, and education); and 3 subjective health measures (days of poor mental health in past month, days of poor physical health in the past month, and having difficulty walking).\nWe will test 3 different predictive models, each using 3 different statistical approaches. Our first model will consider objective indices of health conditions plus demographics (8 variables total). Our second model will retain demographic variables but will replace objective health conditions with health behaviors. Finally, our third model will include all predictors described above, including demographics, health conditions, health behaviors, and perceived health.\nWe will begin with a subset of 70% of the data to train each of our models using logistic regression, classification trees, and random forest plots with 5-fold cross-validation. Then, we will apply our models to the remaining 30% of the data to test which model does the best at predicting our diabetes outcome."
  },
  {
    "objectID": "Modeling.html#data-split",
    "href": "Modeling.html#data-split",
    "title": "ST558 Final Project - Modeling",
    "section": "Data Split",
    "text": "Data Split\nWe begin by splitting our data using the caret package. We load necessary libraries, read in our data and create factors as before, and set a seed so that our results will be reproducible. Then the code below will create an index of randomly selected row numbers comprising 70% of the data set. We can use this index to subset the identified 70% of rows into a training data set, and then the remaining 30% of rows into a test data set.\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(Metrics)\n\nset.seed(25)\n\ndata_tbl &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\ndata_factors_tbl &lt;- data_tbl |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary, labels = c(\"No\", \"Yes\")),\n         HighBP = factor(HighBP, labels = c(\"No\", \"Yes\")),\n         HighChol = factor(HighChol, labels = c(\"No\", \"Yes\")),\n         CholCheck = factor(CholCheck, labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, labels = c(\"No\", \"Yes\")),\n         Stroke = factor(Stroke, labels = c(\"No\", \"Yes\")),\n         Heart = factor(HeartDiseaseorAttack, labels = c(\"No\", \"Yes\")),\n         PhysActivity = factor(PhysActivity, labels = c(\"No\", \"Yes\")),\n         Fruits = factor(Fruits, labels = c(\"No\", \"Yes\")),\n         Veggies = factor(Veggies, labels = c(\"No\", \"Yes\")),\n         Alcohol = factor(HvyAlcoholConsump, labels = c(\"No\", \"Yes\")),\n         DiffWalk = factor(DiffWalk, labels = c(\"No\", \"Yes\")),\n         Sex = factor(Sex, labels = c(\"Male\", \"Female\")),\n         Age = factor(Age, labels = c(\"Age18to24\", \"Age25to29\", \"Age30to34\", \"Age35to39\", \"Age40to44\", \"Age45to49\", \"Age50to54\", \"Age55to59\", \"Age60to64\", \"Age65to69\", \"Age70to74\", \"Age75to79\", \"Age80orOlder\")),\n         Education = factor(Education, labels = c(\"NoSchool\", \"Elementary\", \"SomeHS\", \"HSGrad\", \"SomeCollege\", \"CollegeGrad\")))|&gt;\n  select(Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke, Heart, PhysActivity, Fruits, Veggies, Alcohol, MentHlth, PhysHlth, DiffWalk, Sex, Age, Education)\n\ntrainIndex &lt;- createDataPartition(\n  data_factors_tbl$Diabetes_binary, p = 0.7, list = FALSE)\n\ndataTrain &lt;- data_factors_tbl[trainIndex, ]\ndataTest &lt;- data_factors_tbl[-trainIndex, ]"
  },
  {
    "objectID": "Modeling.html#data-modeling",
    "href": "Modeling.html#data-modeling",
    "title": "ST558 Final Project - Modeling",
    "section": "Data Modeling",
    "text": "Data Modeling\nBelow, we will use three different approaches to predict diabetes outcomes within our data set. In order to evaluate our predictive models, we will use logLoss as our metric. LogLoss is technically computed as -1*the log of the likelihood function. In practice, this means: a) calculating the “corrected” probability for each predicted observation, which is equal to the predicted probability for “success” trials and 1 - the predicted probability for “fail” trials; b) taking the log of each “corrected” probability; c) averaging these together; and d) multiplying the result by -1. Intuitively, we can see that for part a, if a predicted probability is closer to the actual outcome (e.g., .9 when outcome = 1 or .1 when outcome = 0), the resulting “corrected” value will be higher (i.e., closer to 1), and thus the log will have a smaller negative value. Taking the average of these and multiplying by -1 results in a metric in which the smaller the number, the better job the model did in coming close to predicting actual outcomes.\nThis approach is preferable to other metrics for logistic regression for several reasons. Mean squared error (MSE) is better suited to continuous outcomes, and is inappropriate for classification. Accuracy can be used for classification, but logLoss provides greater information. Specifically, the corrected probabilities used to compute logLoss provide a weighting for how close the prediction is to the correct answer, whereas accuracy would simply count each prediction as correct or incorrect. As such, logLoss captures additional nuance that is missed with a simple index of accuracy.\n\nLogistic Regression\nLogistic regression falls under the umbrella of generalized linear models, which expand beyond the basic linear regression framework to incorporate non-normal outcome distributions. Logistic regression, in particular, allows us to model the probability of a binary outcome. Logistic regression uses the same building blocks of the general linear model, which means that we are modeling a linear relationship between the parameter estimates (i.e., coefficients) of the predictor variables and the outcome. One of the assumptions of the linear model is that the outcome can take on any value on the real number line, positive or negative (even if those values are not theoretically meaningful). This assumption is clearly violated in the case of a binary outcome, in which the predicted values can only fall between 0 and 1. Thus, in order to model a linear relationship, the probability outcome must be transformed using a link function (specifically a logit function in the case of logistic regression), which removes the constraints of the probability outcome by mapping this onto the entire real number line. This means that we are not directly modeling probability. Instead, the logit function transforms our outcome into log odds.\nIn our dataset, we are interested in predicting the probability of being diagnosed with diabetes as a function of our predictor variables. Our diabetes outcome is binary, and thus logistic regression is an ideal framework for our predictive modeling.\nEach of the models below first sets up the variables we are testing as predictors and the data set these are drawn from. We specify that we are using a generalized linear model (glm) as our method, with a binomial outcome. We can include preprocessing (centering and scaling) within our code. The traincontrol function then allows us to specify that we will use 5-fold cross-validation, and that we will obtain probabilities for each of our predicted outcomes, which will be used to calculate logLoss as our summary metric.\n\nLogistic Regression Model 1: Objective Health Conditions and Demographics Model\n\nlog_fit1 &lt;- train(Diabetes_binary ~ HighBP + HighChol + BMI + Stroke + Heart + Sex + Age + Education, data = dataTrain,\n             method = \"glm\",\n             family = \"binomial\",\n             preProcess = c(\"center\", \"scale\"),\n             trControl = trainControl(method = \"cv\", number = 5, \n                                      classProbs = TRUE, \n                                      summaryFunction = mnLogLoss))\n\nlog_fit1\n\nGeneralized Linear Model \n\n177577 samples\n     8 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (23), scaled (23) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142062, 142061, 142062 \nResampling results:\n\n  logLoss  \n  0.3340395\n\n\n\n\nLogistic Regression Model 2: Health Behaviors and Demographics Model\n\nlog_fit2 &lt;- train(Diabetes_binary ~ Sex + Age + Education + Smoker + Alcohol + PhysActivity + Fruits + Veggies, data = dataTrain,\n             method = \"glm\",\n             family = \"binomial\",\n             preProcess = c(\"center\", \"scale\"),\n             trControl = trainControl(method = \"cv\", number = 5, \n                                      classProbs = TRUE, \n                                      summaryFunction = mnLogLoss))\n\nlog_fit2\n\nGeneralized Linear Model \n\n177577 samples\n     8 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (23), scaled (23) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142062, 142062, 142061 \nResampling results:\n\n  logLoss  \n  0.3712897\n\n\n\n\nLogistic Regression Model 3: Full Model Including Health Conditions, Health Behaviors, Demographics, and Subjective Ratings\n\nlog_fit3 &lt;- train(Diabetes_binary ~ HighBP + HighChol + Stroke + Heart + BMI + Sex + Age + Education + Smoker + Alcohol + PhysActivity + Fruits + Veggies , data = dataTrain,\n             method = \"glm\",\n             family = \"binomial\",\n             preProcess = c(\"center\", \"scale\"),\n             trControl = trainControl(method = \"cv\", number = 5, \n                                      classProbs = TRUE, \n                                      summaryFunction = mnLogLoss))\n\nlog_fit3\n\nGeneralized Linear Model \n\n177577 samples\n    13 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (28), scaled (28) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results:\n\n  logLoss  \n  0.3319427\n\n\n\n\n\nClassification Tree\nFitting a classification tree is another approach to predicting a binomial outcome. In this approach, the model will attempt to minimize how far our predictions deviate from the actual outcome by splitting the data into different “regions”, with different predictions for different regions. The split that minimizes logLoss then serves as the initial “branch” for the tree. From there, additional splits can be made using the same process until an optimal fit is achieved.\nBelow, we set up each model to be tested as before, specify the classification tree method and our logLoss metric, preprocess the data, and specify the same 5-fold cross-validation as we used in our logistic regression models. We then specify our tuning parameters, which we use to vary the complexity of the branching decisions in our model. Specifically, a smaller value for complexity will allow for more branching. The tested models will apply cycle through our specified range of complexity values to determine the best fit. (Note that in many cases a broader range of values was initially tested, but what is shown below encompasses the optimal value and any variability that could be seen in logLoss outcomes.)\n\nClassification Tree Model 1: Objective Health Conditions and Demographics Model\n\ntree_fit1 &lt;- train(Diabetes_binary ~ HighBP + HighChol + BMI + Stroke + Heart + Sex + Age + Education, data = dataTrain,\n             method = \"rpart\",\n             metric = \"logLoss\",\n             preProcess = c(\"center\", \"scale\"),\n             trControl = trainControl(method = \"cv\", number = 5, \n                                      classProbs = TRUE, \n                                      summaryFunction = mnLogLoss),\n             tuneGrid = expand.grid(cp = seq(from = .001, to = .01, by = .001)))\n\ntree_fit1\n\nCART \n\n177577 samples\n     8 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (23), scaled (23) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142063, 142061, 142061, 142062, 142061 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.001  0.3576292\n  0.002  0.3669846\n  0.003  0.4037576\n  0.004  0.4037576\n  0.005  0.4037576\n  0.006  0.4037576\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001.\n\n\n\n\nClassification Tree Model 2: Health Behaviors and Demographics Model\n\ntree_fit2 &lt;- train(Diabetes_binary ~ Sex + Age + Education + Smoker + Alcohol + PhysActivity + Fruits + Veggies, data = dataTrain,\n             method = \"rpart\",\n             metric = \"logLoss\",\n             preProcess = c(\"center\", \"scale\"),\n             trControl = trainControl(method = \"cv\", number = 5, \n                                      classProbs = TRUE, \n                                      summaryFunction = mnLogLoss),\n             tuneGrid = expand.grid(cp = seq(from = .001, to = .01, by = .001)))\n\ntree_fit2\n\nCART \n\n177577 samples\n     8 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (23), scaled (23) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142061, 142061, 142062, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.001  0.4037576\n  0.002  0.4037576\n  0.003  0.4037576\n  0.004  0.4037576\n  0.005  0.4037576\n  0.006  0.4037576\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.01.\n\n\n\n\nClassification Tree Model 3: Full Model Including Health Conditions, Health Behaviors, Demographics, and Subjective Ratings\n\ntree_fit3 &lt;- train(Diabetes_binary ~ HighBP + HighChol + Stroke + Heart + BMI + Sex + Age + Education + Smoker + Alcohol + PhysActivity + Fruits + Veggies + MentHlth + PhysHlth + DiffWalk, data = dataTrain,\n             method = \"rpart\",\n             metric = \"logLoss\",\n             preProcess = c(\"center\", \"scale\"),\n             trControl = trainControl(method = \"cv\", number = 5, \n                                      classProbs = TRUE, \n                                      summaryFunction = mnLogLoss),\n             tuneGrid = expand.grid(cp = seq(from = .001, to = .01, by = .001)))\n\ntree_fit3\n\nCART \n\n177577 samples\n    16 predictor\n     2 classes: 'No', 'Yes' \n\nPre-processing: centered (31), scaled (31) \nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142062, 142062, 142061, 142061, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.001  0.3573446\n  0.002  0.3574422\n  0.003  0.3574710\n  0.004  0.3770003\n  0.005  0.4037576\n  0.006  0.4037576\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001.\n\n\n\n\n\nRandom Forest\nA random forest model is the ensemble version of the classification tree, in that it constructs multiple trees and then aggregates their predictions to form an optimal model. A random forest model of regression trees can use the mean predicted value across models, whereas the aggregate of classification trees will typically use the modal predicted value. Random forests offer an advantage over a single tree because a single tree may easily overfit the data by creating complex branches that are very specific to the training model. By contrast, random forests reduce the variance and thus are less likely to over-fit. Random forests randomly subset the features that are used to identify branching points, which reduces the correlation between individual trees (again reducing the risk of over-fitting). This can be helpful in cases where a particular feature might otherwise dominate an individual decision tree. In the model below, we focus on the full model (Model 3) and specify the number of variables to be randomly selected for each subset as our tuning parameter (mtry), with values ranging from 8 to 12.\n\nRandom Forest Model 3: Full Model Including Health Conditions, Health Behaviors, Demographics, and Subjective Ratings\n\nrf_fit3 &lt;- train(Diabetes_binary ~ HighBP + HighChol + Stroke + Heart + BMI + Sex + Age + Education + Smoker + Alcohol + PhysActivity + Fruits + Veggies + MentHlth + PhysHlth + DiffWalk, data = dataTrain,\n             method = \"rf\",\n             metric = \"logLoss\",\n             family = \"binomial\",\n             ntree = 100,\n             preProcess = c(\"center\", \"scale\"),\n             trControl = trainControl(method = \"cv\", number = 3, \n                                      classProbs = TRUE, \n                                      summaryFunction = mnLogLoss),\n             tuneGrid = data.frame(mtry = 8:12))\n\nrf_fit3"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "ST558 Final Project - Modeling",
    "section": "Final Model Selection",
    "text": "Final Model Selection\nFor our logistic regression models, model 3 achieved a lower logLoss than models 1 and 2, indicating that using the full model with all predictors resulted in better predictions in our training data set. In comparing our classification trees, we see that model 1 and model 3 performed equally well, while model 2 clearly performed worse than the other two. Finally, when using random forest plots,\nWe will now test the top model from each of our statistical approaches using our test data set to determine the winner! Although models 1 and 3 were tied for the best classification tree model, we will select model 3 in order to compare our predictions for this model across all three methods.\nFirst, we use the parameters derived from training each model and use these to predict outcomes with the test data. The code below will output the predicted probability of having diabetes for each observation in the test data using the trained parameters. These predictions will be saved in the named “pred” file in a column labeled “Yes”.\n\nlog_pred &lt;- predict.train(log_fit3, newdata = dataTest, type = \"prob\")\ntree_pred &lt;- predict.train(tree_fit3, newdata = dataTest, type = \"prob\")\n#rf_pred &lt;- predict.train(rf_fit3, newdata = dataTest, type = \"prob\")\n\nNext, we convert the Diabetes_binary variable within the test data set to a numeric variable that takes on values of 0 (no diabetes) or 1 (diabetes) to use for calculating logLoss with the Metrics package. For each model, we include the numeric test data variable and the predicted “Yes” column to obtain this value.\n\ndataTest &lt;- dataTest |&gt;\n  mutate(Diabetes_num = as.numeric(ifelse(Diabetes_binary == \"Yes\", 1, 0)))\n\n\nLogLoss for Test Data Predictions\n\nLogistic Regression Model\n\nlogLoss(dataTest$Diabetes_num, log_pred$Yes)\n\n[1] 0.3335727\n\n\n\n\nClassification Tree Model\n\nlogLoss(dataTest$Diabetes_num, tree_pred$Yes)\n\n[1] 0.3578572\n\n\n\n\nRandom Forest Model\n\n#logLoss(dataTest$Diabetes_num, rf_pred$Yes)"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "ST558 Final Project - EDA",
    "section": "",
    "text": "In these analyses, we will explore and model data on risk factors for diabetes from the 2015 Behavioral Risk Factor Surveillance System (BRFSS). Data are collected annually by phone using random digit dialing across all 50 states, resulting in over 250,000 adults over age 18 providing information about health conditions, health behaviors, and healthcare use. For the current analyses, we focus on the curated binary Diabetes Health Indicator Dataset. This dataset includes the variable “Diabetes_binary”, which is coded as 1 to indicate a respondent has been told they have diabetes, and 0 if they have not. This variable will serve as the primary outcome for our predictions.\nThe Diabetes Health Indicator Dataset also includes a subset of additional variables selected from the broader survey based on their relevance to diabetes risk, which we will use for our predictive models. These include the following:\n\nOther binary physical health indicators (1 = yes, 0 = no)\n\nreported diagnosis of high blood pressure\nreported diagnosis of high cholesterol\nwhether cholesterol has ever been checked\nhistory of stroke\nhistory of coronary heart disease or myocardial infarction);\n\nA continuous measure of body mass index (BMI)\nBinary indicators of healthy or unhealthy behaviors (1 = yes, 0 = no)\n\neating at least 1 fruit per day\neating at least 1 vegetable per day\ndoing any exercise beyond normal activity in the past 30 days\nsmoking &gt; 100 cigarettes lifetime\ndrinking &gt; 14 drinks/week for men or 7 drinks/week for women)\n\nCategorical demographic variables\n\nsex (1 = Male, 2 = Female)\nage group (13 possible categories divided into 5-year bins ranging from 1 = (18 to 24) to 13 = (age 80 or older))\nhighest level of education completed (6 categories ranging from never attended school to collge graduate)\n\nSubjective ratings of health\n\nnumber of days in the past 30 in which physical health was not good\nnumber of days in the past 30 in which mental health was not good\na binary indicator of difficulty walking or climbing stairs.\n\n\nIn the steps below, we will conduct exploratory data analysis to identify missing values, examine the distributions of our predictor variables, and explore preliminary associations between predictor variables and the diabetes outcome."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "ST558 Final Project - EDA",
    "section": "",
    "text": "In these analyses, we will explore and model data on risk factors for diabetes from the 2015 Behavioral Risk Factor Surveillance System (BRFSS). Data are collected annually by phone using random digit dialing across all 50 states, resulting in over 250,000 adults over age 18 providing information about health conditions, health behaviors, and healthcare use. For the current analyses, we focus on the curated binary Diabetes Health Indicator Dataset. This dataset includes the variable “Diabetes_binary”, which is coded as 1 to indicate a respondent has been told they have diabetes, and 0 if they have not. This variable will serve as the primary outcome for our predictions.\nThe Diabetes Health Indicator Dataset also includes a subset of additional variables selected from the broader survey based on their relevance to diabetes risk, which we will use for our predictive models. These include the following:\n\nOther binary physical health indicators (1 = yes, 0 = no)\n\nreported diagnosis of high blood pressure\nreported diagnosis of high cholesterol\nwhether cholesterol has ever been checked\nhistory of stroke\nhistory of coronary heart disease or myocardial infarction);\n\nA continuous measure of body mass index (BMI)\nBinary indicators of healthy or unhealthy behaviors (1 = yes, 0 = no)\n\neating at least 1 fruit per day\neating at least 1 vegetable per day\ndoing any exercise beyond normal activity in the past 30 days\nsmoking &gt; 100 cigarettes lifetime\ndrinking &gt; 14 drinks/week for men or 7 drinks/week for women)\n\nCategorical demographic variables\n\nsex (1 = Male, 2 = Female)\nage group (13 possible categories divided into 5-year bins ranging from 1 = (18 to 24) to 13 = (age 80 or older))\nhighest level of education completed (6 categories ranging from never attended school to collge graduate)\n\nSubjective ratings of health\n\nnumber of days in the past 30 in which physical health was not good\nnumber of days in the past 30 in which mental health was not good\na binary indicator of difficulty walking or climbing stairs.\n\n\nIn the steps below, we will conduct exploratory data analysis to identify missing values, examine the distributions of our predictor variables, and explore preliminary associations between predictor variables and the diabetes outcome."
  },
  {
    "objectID": "EDA.html#preparing-data",
    "href": "EDA.html#preparing-data",
    "title": "ST558 Final Project - EDA",
    "section": "Preparing Data",
    "text": "Preparing Data\nFirst, we will load necessary packages and read in the Diabetes Health Indicators Dataset.\n\nlibrary(tidyverse)\n\ndata_tbl &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nhead(data_tbl)\n\n# A tibble: 6 × 22\n  Diabetes_binary HighBP HighChol CholCheck   BMI Smoker Stroke\n            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1               0      1        1         1    40      1      0\n2               0      0        0         0    25      1      0\n3               0      1        1         1    28      0      0\n4               0      1        0         1    27      0      0\n5               0      1        1         1    24      0      0\n6               0      1        1         1    25      1      0\n# ℹ 15 more variables: HeartDiseaseorAttack &lt;dbl&gt;, PhysActivity &lt;dbl&gt;,\n#   Fruits &lt;dbl&gt;, Veggies &lt;dbl&gt;, HvyAlcoholConsump &lt;dbl&gt;, AnyHealthcare &lt;dbl&gt;,\n#   NoDocbcCost &lt;dbl&gt;, GenHlth &lt;dbl&gt;, MentHlth &lt;dbl&gt;, PhysHlth &lt;dbl&gt;,\n#   DiffWalk &lt;dbl&gt;, Sex &lt;dbl&gt;, Age &lt;dbl&gt;, Education &lt;dbl&gt;, Income &lt;dbl&gt;\n\n\nNext, we will determine the number of valid observations versus missing data. We see below that there are no NA values across any of the variables.\n\ncolSums(is.na(data_tbl))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\nHowever, this is not surprising as the BRFSS surveys contain item-specific codes for responses of “don’t know” or “refused to answer”. To probe how often these codes are used, we convert the categorical variables we are interest in into factors and then examine the structure to determine if categories representing missing data are being used.\n\ndata_factors_tbl &lt;- data_tbl |&gt;\n  mutate(Diabetes_binary = as.factor(Diabetes_binary),\n         HighBP = as.factor(HighBP),\n         HighChol = as.factor(HighChol),\n         CholCheck = as.factor(CholCheck),\n         Smoker = as.factor(Smoker),\n         Stroke = as.factor(Stroke),\n         Heart = as.factor(HeartDiseaseorAttack),\n         PhysActivity = as.factor(PhysActivity),\n         Fruits = as.factor(Fruits),\n         Veggies = as.factor(Veggies),\n         Alcohol = as.factor(HvyAlcoholConsump),\n         DiffWalk = as.factor(DiffWalk),\n         Sex = as.factor(Sex),\n         Age = as.factor(Age),\n         Education = as.factor(Education)) |&gt;\n  select(Diabetes_binary, HighBP, HighChol, CholCheck, Smoker, Stroke, Heart, PhysActivity, Fruits, Veggies, Alcohol, DiffWalk, Sex, Age, Education)\n\nstr(data_factors_tbl)\n\ntibble [253,680 × 15] (S3: tbl_df/tbl/data.frame)\n $ Diabetes_binary: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP         : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol       : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 2 2 1 2 2 1 ...\n $ CholCheck      : Factor w/ 2 levels \"0\",\"1\": 2 1 2 2 2 2 2 2 2 2 ...\n $ Smoker         : Factor w/ 2 levels \"0\",\"1\": 2 2 1 1 1 2 2 2 2 1 ...\n $ Stroke         : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Heart          : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 2 1 ...\n $ PhysActivity   : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 2 2 1 2 1 1 ...\n $ Fruits         : Factor w/ 2 levels \"0\",\"1\": 1 1 2 2 2 2 1 1 2 1 ...\n $ Veggies        : Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 2 2 1 2 2 2 ...\n $ Alcohol        : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ DiffWalk       : Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex            : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age            : Factor w/ 13 levels \"1\",\"2\",\"3\",\"4\",..: 9 7 9 11 11 10 9 11 9 8 ...\n $ Education      : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 4 6 4 3 5 6 6 4 5 4 ...\n\n\nAbove, we see from looking at the structure of the data set that our binary variables include only 0 or 1 as possible levels. This leaves BMI, MentHlth, and PhysHlth as continuous variables, and Age and Education as factors with &gt; 2 categories.\nWe can use plotting to explore the frequencies of categorical variables with &gt; 2 levels. We see below that all values fall within the expected range of coded categories, indicating that the data set has been fully cleaned and includes only respondents with complete data.\n\ng &lt;- ggplot(data_factors_tbl) + geom_bar()\n\ng + aes(x = Age)\n\n\n\ng + aes(x = Education)\n\n\n\n\nBefore continuing on, we will return to our factors and recode them to have meaningful levels, which will also eliminate errors related to factor level naming in our modeling steps.\n\ndata_factors_tbl &lt;- data_tbl |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary, labels = c(\"No\", \"Yes\")),\n         HighBP = factor(HighBP, labels = c(\"No\", \"Yes\")),\n         HighChol = factor(HighChol, labels = c(\"No\", \"Yes\")),\n         CholCheck = factor(CholCheck, labels = c(\"No\", \"Yes\")),\n         Smoker = factor(Smoker, labels = c(\"No\", \"Yes\")),\n         Stroke = factor(Stroke, labels = c(\"No\", \"Yes\")),\n         Heart = factor(HeartDiseaseorAttack, labels = c(\"No\", \"Yes\")),\n         PhysActivity = factor(PhysActivity, labels = c(\"No\", \"Yes\")),\n         Fruits = factor(Fruits, labels = c(\"No\", \"Yes\")),\n         Veggies = factor(Veggies, labels = c(\"No\", \"Yes\")),\n         Alcohol = factor(HvyAlcoholConsump, labels = c(\"No\", \"Yes\")),\n         DiffWalk = factor(DiffWalk, labels = c(\"No\", \"Yes\")),\n         Sex = factor(Sex, labels = c(\"Male\", \"Female\")),\n         Age = factor(Age, labels = c(\"Age18to24\", \"Age25to29\", \"Age30to34\", \"Age35to39\", \"Age40to44\", \"Age45to49\", \"Age50to54\", \"Age55to59\", \"Age60to64\", \"Age65to69\", \"Age70to74\", \"Age75to79\", \"Age80orOlder\")),\n         Education = factor(Education, labels = c(\"NoSchool\", \"Elementary\", \"SomeHS\", \"HSGrad\", \"SomeCollege\", \"CollegeGrad\")))|&gt;\n  select(Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke, Heart, PhysActivity, Fruits, Veggies, Alcohol, MentHlth, PhysHlth, DiffWalk, Sex, Age, Education)"
  },
  {
    "objectID": "EDA.html#summarizing-data",
    "href": "EDA.html#summarizing-data",
    "title": "ST558 Final Project - EDA",
    "section": "Summarizing Data",
    "text": "Summarizing Data\n\nDistributions of Continuous Variables\nNow that we have our data structured in the way we want, We can begin to explore our data. First, we examine summary statistics for continuous variables. We see below that all values for MentHlth and PhysHlth fall within the expected range of 0 to 30, whereas the maximum BMI value is 98, which seems biologically implausible.\n\ndata_factors_tbl |&gt;\n  summarize(across(where(is.numeric), \n                   list(\"mean\" = ~ mean(.x), \n                        \"max\" = ~ max(.x),\n                        \"min\" = ~ min(.x)),\n                   .names = \"{.fn}_{.col}\"))\n\n# A tibble: 1 × 9\n  mean_BMI max_BMI min_BMI mean_MentHlth max_MentHlth min_MentHlth mean_PhysHlth\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n1     28.4      98      12          3.18           30            0          4.24\n# ℹ 2 more variables: max_PhysHlth &lt;dbl&gt;, min_PhysHlth &lt;dbl&gt;\n\n\nWe can view the entire distribution of BMI using a histogram. We see below that values above ~ 60 are extremely rare. Given that there is not a spike in values of 98, this suggests that this is not being used as some sort of system code for missing data.\n\nggplot(data = data_factors_tbl, aes(x = BMI)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nBelow we see that there are 53 people with BMI &gt; 90 and 7 with a BMI of 98. As such, it may be that these values represent rare but true values at the outer limits of the possible range for people with severe morbid obesity.\n\ndata_factors_tbl |&gt;\n  count(BMI &gt; 90) \n\n# A tibble: 2 × 2\n  `BMI &gt; 90`      n\n  &lt;lgl&gt;       &lt;int&gt;\n1 FALSE      253627\n2 TRUE           53\n\ndata_factors_tbl |&gt;\n  count(BMI &gt; 97)\n\n# A tibble: 2 × 2\n  `BMI &gt; 97`      n\n  &lt;lgl&gt;       &lt;int&gt;\n1 FALSE      253673\n2 TRUE            7\n\n\nWe can also view MentHlth and PhysHlth in this way. We see that for both variables, the vast majority of respondents (~160,000 and 175,000, respectively) say they had 0 out of 30 days in which their health was not good, while a sizeable subset (~12,000 and 19,000, respectively) indicate their health was poor across all 30 days.\n\nggplot(data = data_factors_tbl, aes(x = MentHlth)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nggplot(data = data_factors_tbl, aes(x = PhysHlth)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\ndata_factors_tbl |&gt;\n  count(PhysHlth == 0)\n\n# A tibble: 2 × 2\n  `PhysHlth == 0`      n\n  &lt;lgl&gt;            &lt;int&gt;\n1 FALSE            93628\n2 TRUE            160052\n\ndata_factors_tbl |&gt;\n  count(PhysHlth == 30)\n\n# A tibble: 2 × 2\n  `PhysHlth == 30`      n\n  &lt;lgl&gt;             &lt;int&gt;\n1 FALSE            234280\n2 TRUE              19400\n\ndata_factors_tbl |&gt;\n  count(MentHlth == 0)\n\n# A tibble: 2 × 2\n  `MentHlth == 0`      n\n  &lt;lgl&gt;            &lt;int&gt;\n1 FALSE            78000\n2 TRUE            175680\n\ndata_factors_tbl |&gt;\n  count(MentHlth == 30)\n\n# A tibble: 2 × 2\n  `MentHlth == 30`      n\n  &lt;lgl&gt;             &lt;int&gt;\n1 FALSE            241592\n2 TRUE              12088\n\n\n\n\nFrequencies of Binary Outcomes\nNext, we turn to our categorical variables. We can look at the frequencies of our binary outcomes to get a sense of the prevalence of diabetes and each of the risk factors.\n\ng + aes(x = Diabetes_binary) \n\n\n\ndata_factors_tbl |&gt;\n  count(Diabetes_binary)\n\n# A tibble: 2 × 2\n  Diabetes_binary      n\n  &lt;fct&gt;            &lt;int&gt;\n1 No              218334\n2 Yes              35346\n\ng + aes(x = HighBP)\n\n\n\ng + aes(x = HighChol)\n\n\n\ng + aes(x = CholCheck)\n\n\n\ng + aes(x = Smoker)\n\n\n\ng + aes(x = Stroke)\n\n\n\ng + aes(x = Alcohol)\n\n\n\ng + aes(x = Heart)\n\n\n\ng + aes(x = PhysActivity)\n\n\n\ng + aes(x = Fruits)\n\n\n\ng + aes(x = Veggies)\n\n\n\ng + aes(x = DiffWalk)\n\n\n\ng + aes(x = Sex)\n\n\n\n\nWe can see that some risk factors are more common than others. High blood pressure, high cholesterol, and history of smoking &gt; 100 cigarettes are extremely common, whereas history of stroke or heart disease are far less common.\n\n\nPrevalence of Diabetes as a Function of Risk Factors\n\nObjective Health Conditions\nIt is likely that many of these risk factors co-occur. Below, we create contingency tables to examine preliminary associations between predictors and with diabetes outcome.\n\ndata_factors_tbl |&gt;\n  group_by(HighBP, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'HighBP'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   HighBP [2]\n  HighBP     No   Yes\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt;\n1 No     136109  8742\n2 Yes     82225 26604\n\ndata_factors_tbl |&gt;\n  group_by(HighChol, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'HighChol'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   HighChol [2]\n  HighChol     No   Yes\n  &lt;fct&gt;     &lt;int&gt; &lt;int&gt;\n1 No       134429 11660\n2 Yes       83905 23686\n\ndata_factors_tbl |&gt;\n  group_by(Stroke, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'Stroke'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Stroke [2]\n  Stroke     No   Yes\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt;\n1 No     211310 32078\n2 Yes      7024  3268\n\ndata_factors_tbl |&gt;\n  group_by(Heart, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'Heart'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Heart [2]\n  Heart     No   Yes\n  &lt;fct&gt;  &lt;int&gt; &lt;int&gt;\n1 No    202319 27468\n2 Yes    16015  7878\n\ndata_factors_tbl |&gt;\n  group_by(HighBP, HighChol) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = HighChol, values_from = count)\n\n`summarise()` has grouped output by 'HighBP'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   HighBP [2]\n  HighBP     No   Yes\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt;\n1 No     101920 42931\n2 Yes     44169 64660\n\ndata_factors_tbl |&gt;\n  group_by(Stroke, Heart) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Heart, values_from = count)\n\n`summarise()` has grouped output by 'Stroke'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Stroke [2]\n  Stroke     No   Yes\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt;\n1 No     223432 19956\n2 Yes      6355  3937\n\n\nWe can see from the tables above that the majority of people with diabetes also have high blood pressure and high cholesterol. Moreover, the majority of people with high blood pressure also have high cholesterol, and vice versa. Heart disease and stroke are not endorsed by the majority of respondents with diabetes, likely due to their lower overall prevalence, but the pattern of responses supports their status as risk factors.\nOne additional check we can make with health conditions is to examine how often not having high cholesterol is due to never having this checked.\n\ndata_factors_tbl |&gt;\n  group_by(CholCheck, HighChol) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = HighChol, values_from = count)\n\n`summarise()` has grouped output by 'CholCheck'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   CholCheck [2]\n  CholCheck     No    Yes\n  &lt;fct&gt;      &lt;int&gt;  &lt;int&gt;\n1 No          7489   1981\n2 Yes       138600 105610\n\n\nWe can see that the vast majority of people who report not having high cholesterol do indicate that they have had their cholesterol checked. It is interesting that 1981 people report that they have high cholesterol despite also indicating that they have never had their cholesterol checked, which is difficult to interpret.\n\n\nHealth Behaviors\nNext, we look at health behaviors.\n\ndata_factors_tbl |&gt;\n  group_by(Smoker, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'Smoker'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Smoker [2]\n  Smoker     No   Yes\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt;\n1 No     124228 17029\n2 Yes     94106 18317\n\ndata_factors_tbl |&gt;\n  group_by(Alcohol, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'Alcohol'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Alcohol [2]\n  Alcohol     No   Yes\n  &lt;fct&gt;    &lt;int&gt; &lt;int&gt;\n1 No      204910 34514\n2 Yes      13424   832\n\ndata_factors_tbl |&gt;\n  group_by(PhysActivity, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'PhysActivity'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   PhysActivity [2]\n  PhysActivity     No   Yes\n  &lt;fct&gt;         &lt;int&gt; &lt;int&gt;\n1 No            48701 13059\n2 Yes          169633 22287\n\ndata_factors_tbl |&gt;\n  group_by(Veggies, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'Veggies'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Veggies [2]\n  Veggies     No   Yes\n  &lt;fct&gt;    &lt;int&gt; &lt;int&gt;\n1 No       39229  8610\n2 Yes     179105 26736\n\ndata_factors_tbl |&gt;\n  group_by(Fruits, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'Fruits'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Fruits [2]\n  Fruits     No   Yes\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt;\n1 No      78129 14653\n2 Yes    140205 20693\n\n\nHistory of smoking and low physical activity appear to be more common in people with diabetes, but in general the patterns are not as striking as those of high blood pressure and cholesterol.\n\ndata_factors_tbl |&gt;\n  group_by(Smoker, Alcohol) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Alcohol, values_from = count)\n\n`summarise()` has grouped output by 'Smoker'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Smoker [2]\n  Smoker     No   Yes\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt;\n1 No     136268  4989\n2 Yes    103156  9267\n\ndata_factors_tbl |&gt;\n  group_by(Smoker, PhysActivity) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = PhysActivity, values_from = count)\n\n`summarise()` has grouped output by 'Smoker'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Smoker [2]\n  Smoker    No    Yes\n  &lt;fct&gt;  &lt;int&gt;  &lt;int&gt;\n1 No     29663 111594\n2 Yes    32097  80326\n\ndata_factors_tbl |&gt;\n  group_by(Fruits, Veggies) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Veggies, values_from = count)\n\n`summarise()` has grouped output by 'Fruits'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   Fruits [2]\n  Fruits    No    Yes\n  &lt;fct&gt;  &lt;int&gt;  &lt;int&gt;\n1 No     29653  63129\n2 Yes    18186 142712\n\n\nExploring contingencies between risk factors, we see that people who drink heavily and people with low physical activity are more likely to have smoked &gt; 100 cigarettes. In addition, people who don’t eat veggies are much more likely to not eat fruits and vice versa.\n\n\nSubjective Health Measures\nWe can also look at frequency of diabetes as a function of subjective health. To simplify results, we’ll look at the extremes (0 vs 30) of perceived mental and physical health.\n\ndata_factors_tbl |&gt;\n  group_by(DiffWalk, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'DiffWalk'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   DiffWalk [2]\n  DiffWalk     No   Yes\n  &lt;fct&gt;     &lt;int&gt; &lt;int&gt;\n1 No       188780 22225\n2 Yes       29554 13121\n\ndata_factors_tbl |&gt;\n  filter(MentHlth == 0 | MentHlth == 30) |&gt;\n  group_by(MentHlth, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'MentHlth'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   MentHlth [2]\n  MentHlth     No   Yes\n     &lt;dbl&gt;  &lt;int&gt; &lt;int&gt;\n1        0 152277 23403\n2       30   9320  2768\n\ndata_factors_tbl |&gt;\n  filter(PhysHlth == 0 | PhysHlth == 30) |&gt;\n  group_by(PhysHlth, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count)\n\n`summarise()` has grouped output by 'PhysHlth'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 2 × 3\n# Groups:   PhysHlth [2]\n  PhysHlth     No   Yes\n     &lt;dbl&gt;  &lt;int&gt; &lt;int&gt;\n1        0 143312 16740\n2       30  13674  5726\n\n\nNot surprising, it appears that diabetes is highly prevalent (25-35%) among those with difficulty walking or daily poor perceived health. The prevalence among people endorsing daily poor mental health also appears to be slightly elevated, but this is less striking than poor physical health.\n\n\nDemographic Variables\nFinally, we view the prevalence of diabetes as a function of age, sex, and education level. We see below that the prevalence of diabetes increases with age and decreases at higher levels of education. Prevalence of diabetes is approximately 15% for females and 13% for males.\n\nage &lt;- data_factors_tbl |&gt;\n  group_by(Age, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count) |&gt;\n  mutate(percent = Yes/(Yes + No))\n\n`summarise()` has grouped output by 'Age'. You can override using the `.groups`\nargument.\n\nage \n\n# A tibble: 13 × 4\n# Groups:   Age [13]\n   Age             No   Yes percent\n   &lt;fct&gt;        &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 Age18to24     5622    78  0.0137\n 2 Age25to29     7458   140  0.0184\n 3 Age30to34    10809   314  0.0282\n 4 Age35to39    13197   626  0.0453\n 5 Age40to44    15106  1051  0.0650\n 6 Age45to49    18077  1742  0.0879\n 7 Age50to54    23226  3088  0.117 \n 8 Age55to59    26569  4263  0.138 \n 9 Age60to64    27511  5733  0.172 \n10 Age65to69    25636  6558  0.204 \n11 Age70to74    18392  5141  0.218 \n12 Age75to79    12577  3403  0.213 \n13 Age80orOlder 14154  3209  0.185 \n\nsex &lt;- data_factors_tbl |&gt;\n  group_by(Sex, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count) |&gt;\n  mutate(percent = Yes/(Yes + No))\n\n`summarise()` has grouped output by 'Sex'. You can override using the `.groups`\nargument.\n\nsex\n\n# A tibble: 2 × 4\n# Groups:   Sex [2]\n  Sex        No   Yes percent\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Male   123563 18411   0.130\n2 Female  94771 16935   0.152\n\neduc &lt;- data_factors_tbl |&gt;\n  group_by(Education, Diabetes_binary) |&gt;\n  summarize(count = n()) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = count) |&gt;\n  mutate(percent = Yes/(Yes + No))\n\n`summarise()` has grouped output by 'Education'. You can override using the\n`.groups` argument.\n\neduc\n\n# A tibble: 6 × 4\n# Groups:   Education [6]\n  Education      No   Yes percent\n  &lt;fct&gt;       &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n1 NoSchool      127    47  0.270 \n2 Elementary   2860  1183  0.293 \n3 SomeHS       7182  2296  0.242 \n4 HSGrad      51684 11066  0.176 \n5 SomeCollege 59556 10354  0.148 \n6 CollegeGrad 96925 10400  0.0969\n\n\nNext, we will turn to modeling our data and examining our ability to predict diabetes outcomes with our risk factor variables.\nClick here for the modeling page."
  }
]